{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravitejathota799/ML_LAB_INTERNAL-EXTERNAL/blob/main/All_programs_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIND-S ALGORITHM ExpNO-1"
      ],
      "metadata": {
        "id": "9F5RhUyMNR0W"
      },
      "id": "9F5RhUyMNR0W"
    },
    {
      "cell_type": "code",
      "source": [
        "#FIND-S algorithm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "data=pd.read_csv('Book2.csv')\n",
        "#data.drop(data.columns[0], axis=1, inplace=True)\n",
        "\n",
        "d = np.array(data)[:,:-1]\n",
        "print(d)\n",
        "#print(len(d))\n",
        "target=np.array(data)[:,-1]\n",
        "print(target)\n",
        "\n",
        "def train(a,t):\n",
        "    for i,val in enumerate(t):\n",
        "        if val=='Yes':\n",
        "            s_h=a[i].copy()\n",
        "            break\n",
        "    for i, val in enumerate(a):\n",
        "        if t[i] == \"Yes\":\n",
        "            for x in range(len(s_h)):\n",
        "                if val[x] != s_h[x]:\n",
        "                    s_h[x] = '?'\n",
        "                else:\n",
        "                    pass\n",
        "                 \n",
        "            print(s_h)\n",
        "train(d,target)\n"
      ],
      "metadata": {
        "id": "Suu4TljHNV5Q"
      },
      "id": "Suu4TljHNV5Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Candidate Elimination Algorithm ExpNo-2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nKUN5FLiNdzu"
      },
      "id": "nKUN5FLiNdzu"
    },
    {
      "cell_type": "code",
      "source": [
        "#cnadidate Elimination\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_csv('Book2.csv')\n",
        "#data.drop(data.columns[0], axis=1, inplace=True)#removed Example Column from dataset\n",
        "concepts = np.array(data.iloc[:,0:-1])\n",
        "print(\"\\nInstances are:\\n\",concepts)\n",
        "target = np.array(data.iloc[:,-1])\n",
        "print(\"\\nTarget Values are: \",target)\n",
        "def learning(data,target):\n",
        "    specific_h = concepts[0].copy()\n",
        "    print(\"\\nInitialization of specific_h and genearal_h\")\n",
        "    print(\"\\nSpecific Boundary: \", specific_h)\n",
        "    general_h = [[\"?\" for i in range(len(specific_h))] for i in range(len(specific_h))]\n",
        "    print(\"\\nGeneric Boundary: \",general_h) \n",
        "    for i, h in enumerate(concepts):\n",
        "        print(\"\\nInstance\", i+1 , \"is \", h)\n",
        "        if target[i] == \"Yes\":\n",
        "            print(\"Instance is Positive \")\n",
        "            for x in range(len(specific_h)): \n",
        "                if h[x]!= specific_h[x]:                    \n",
        "                    specific_h[x] ='?'                     \n",
        "                    general_h[x][x] ='?'\n",
        "        if target[i]=='No':\n",
        "            print(\"Instance is Negative\")\n",
        "            for x in range(len(specific_h)):\n",
        "                if h[x]!=specific_h[x]:\n",
        "                    general_h[x][x]=specific_h[x]\n",
        "                else:\n",
        "                    general_h[x][x]='?'\n",
        "            \n",
        "    \n",
        "    idx=[i for i,val in enumerate(general_h) if val==['?','?','?','?','?','?']]\n",
        "    for i in idx:\n",
        "              general_h.remove(['?','?','?','?','?','?'])\n",
        "    print(\"Final specific_h\",specific_h)\n",
        "    print(\"Final general_h\",general_h)\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "learning(data,target)\n"
      ],
      "metadata": {
        "id": "RE9FyCXMNktN"
      },
      "id": "RE9FyCXMNktN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Classification ExpNo-6"
      ],
      "metadata": {
        "id": "P_iksMR2NpAo"
      },
      "id": "P_iksMR2NpAo"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "df=pd.read_csv('txtclassification.csv')\n",
        "df.head()\n",
        "X=df.Text\n",
        "Y=df.Class \n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.4)\n",
        "print(X_train)\n",
        "print(Y_train)\n",
        "vect=CountVectorizer()\n",
        "vect.fit(X_train)\n",
        "X_train_dt = vect.transform(X_train)\n",
        "X_test_dt = vect.transform(X_test)\n",
        "nb=MultinomialNB()\n",
        "nb.fit(X_train_dt, Y_train)\n",
        "Y_pred_class = nb.predict(X_test_dt)\n",
        "acc=metrics.accuracy_score(Y_test, Y_pred_class)\n",
        "print(\"accuracy:\",acc)\n",
        "pre=metrics.precision_score(Y_test, Y_pred_class,average='weighted')\n",
        "print(\"precision:\",pre)\n",
        "rec=metrics.recall_score(Y_test, Y_pred_class,average='weighted')\n",
        "print(\"recall:\",rec)\n",
        "metrics.confusion_matrix(Y_test, Y_pred_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw6NGJfJOHC5",
        "outputId": "95492830-8e1b-4c81-d169-548d02df2e9d"
      },
      "id": "Pw6NGJfJOHC5",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10               This is an awesome place\n",
            "12                        I love to dance\n",
            "4                    What an awesome view\n",
            "6                I am tired of this stuff\n",
            "11    I dont like the taste of this juice\n",
            "16         we will have good fun tomorrow\n",
            "13     I am stick and tired of this place\n",
            "17        I went to my enemys house today\n",
            "14                   what a great holiday\n",
            "9                     My boss is horrible\n",
            "Name: Text, dtype: object\n",
            "10    pos\n",
            "12    pos\n",
            "4     pos\n",
            "6     neg\n",
            "11    neg\n",
            "16    pos\n",
            "13    neg\n",
            "17    neg\n",
            "14    pos\n",
            "9     neg\n",
            "Name: Class, dtype: object\n",
            "accuracy: 0.875\n",
            "precision: 0.90625\n",
            "recall: 0.875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0],\n",
              "       [1, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*KNN* ExpNo-9"
      ],
      "metadata": {
        "id": "_Ue_sYvNOWUa"
      },
      "id": "_Ue_sYvNOWUa"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import *\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Loading data\n",
        "irisData = load_iris()\n",
        "# Create feature and target arrays\n",
        "X = irisData.data\n",
        "y = irisData.target\n",
        "# Split into training and test set\n",
        "X_train, X_test,y_train,y_test = map(np.array,train_test_split(X,y, test_size=0.20))\n",
        "print(f'size of training data {X_train.shape}')\n",
        "print(f'size of testing data {X_test.shape}')\n",
        "print('label 0: Setosa')\n",
        "print('label 1: Versicolor')\n",
        "print('label 2: Virginica')\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "classifier.fit(X_train,y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "print('results of classification with k=1')\n",
        "for i in range(len(X_test)):\n",
        "    print(f'sample: {X_test[i]}\\tpredicted-label: {classifier.predict([X_test[i]])[0]} \\t Actual-labelio : {y_test[i]}')\n",
        "\n",
        "print('confusion matrix')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "2b_LAkuxOYCp"
      },
      "id": "2b_LAkuxOYCp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID3 ExpN0-3"
      ],
      "metadata": {
        "id": "CJdZBytSO9i7"
      },
      "id": "CJdZBytSO9i7"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "df_tennis=pd.read_csv(\"id3.csv\")\n",
        "print(df_tennis)\n",
        "def entropy(probs):\n",
        "    return sum( [-prob*math.log(prob, 2) for prob in probs] )\n",
        "\n",
        "\n",
        "def entropy_of_list(a_list):  \n",
        "    cnt = Counter(x for x in a_list)\n",
        "    num_instances = len(a_list)*1.0\n",
        "    probs = [x / num_instances for x in cnt.values()]  # x means no of YES/NO\n",
        "    \n",
        "    return entropy(probs) # Call Entropy :\n",
        "    \n",
        "\n",
        "total_entropy = entropy_of_list(df_tennis['Play Tennis'])\n",
        "\n",
        "print(\"\\n Total Entropy of PlayTennis Data Set:\",total_entropy)\n",
        "def information_gain(df, split_attribute_name, target_attribute_name, trace=0):\n",
        "    \n",
        "    df_split = df.groupby(split_attribute_name)\n",
        "   \n",
        "    nobs = len(df.index) * 1.0\n",
        "   \n",
        "    df_agg_ent = df_split.agg({target_attribute_name : [entropy_of_list, lambda x: len(x)/nobs] })[target_attribute_name]\n",
        "    \n",
        "    df_agg_ent.columns = ['Entropy', 'PropObservations']\n",
        "    \n",
        "    new_entropy = sum( df_agg_ent['Entropy'] * df_agg_ent['PropObservations'] )\n",
        "    old_entropy = entropy_of_list(df[target_attribute_name])\n",
        "    return old_entropy - new_entropy\n",
        "\n",
        "\n",
        "print('Info-gain for Outlook is :'+str( information_gain(df_tennis, 'Outlook', 'Play Tennis')),\"\\n\")\n",
        "print('\\n Info-gain for Humidity is: ' + str( information_gain(df_tennis, 'Humidity', 'Play Tennis')),\"\\n\")\n",
        "print('\\n Info-gain for Wind is:' + str( information_gain(df_tennis, 'Wind', 'Play Tennis')),\"\\n\")\n",
        "print('\\n Info-gain for Temperature is:' + str( information_gain(df_tennis, 'Temperature','Play Tennis')),\"\\n\")\n",
        "\n",
        "def id3(df, target_attribute_name, attribute_names, default_class=None):\n",
        "    \n",
        "    \n",
        "    from collections import Counter\n",
        "    cnt = Counter(x for x in df[target_attribute_name])\n",
        "    \n",
        "    \n",
        "    if len(cnt) == 1:\n",
        "        return next(iter(cnt))  \n",
        "    \n",
        "    \n",
        "    elif df.empty or (not attribute_names):\n",
        "        return default_class \n",
        "    \n",
        "    \n",
        "    else:\n",
        "        \n",
        "        default_class = max(cnt.keys()) \n",
        "        gainz = [information_gain(df, attr, target_attribute_name) for attr in attribute_names] #\n",
        "        index_of_max = gainz.index(max(gainz)) \n",
        "        best_attr = attribute_names[index_of_max]\n",
        "        \n",
        "        \n",
        "        tree = {best_attr:{}}  \n",
        "        remaining_attribute_names = [i for i in attribute_names if i != best_attr]\n",
        "        \n",
        "        \n",
        "        for attr_val, data_subset in df.groupby(best_attr):\n",
        "            subtree = id3(data_subset,\n",
        "                        target_attribute_name,\n",
        "                        remaining_attribute_names,\n",
        "                        default_class)\n",
        "            tree[best_attr][attr_val] = subtree\n",
        "        return tree\n",
        "\n",
        "attribute_names = list(df_tennis.columns)\n",
        "print(\"List of Attributes:\", attribute_names[1:]) \n",
        "attribute_names.remove('Play Tennis') \n",
        "print(\"Predicting Attributes:\", attribute_names[1:])\n",
        "tree = id3(df_tennis,'Play Tennis',attribute_names[1:])\n",
        "print(\"\\n\\nThe Resultant Decision Tree is :\\n\")\n",
        "print(tree)\n"
      ],
      "metadata": {
        "id": "QiMMQLVdO-6p"
      },
      "id": "QiMMQLVdO-6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes Classifier ExpNo-5"
      ],
      "metadata": {
        "id": "7WBi9Bx1P4xj"
      },
      "id": "7WBi9Bx1P4xj"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df=pd.read_csv(\"Downloads/NBData.csv\")\n",
        "df.head()\n",
        "df.shape[0]\n",
        "train_df=df.head(int(0.6*df.shape[0]))\n",
        "train_df.head(10)\n",
        "df['Play Tennis'][0]\n",
        "test_df=df.tail(6)\n",
        "test_df.head(10)\n",
        "Train_data=df.sample(frac=0.6,random_state=0)\n",
        "Test_data=df.drop(Train_data.index)\n",
        "att=Train_data.columns.values.tolist()\n",
        "print(att)\n",
        "print(Train_data.shape)\n",
        "list(Train_data.index)\n",
        "county=0\n",
        "n=Train_data.shape[0]\n",
        "for i in list(Train_data.index):\n",
        "    if Train_data[\"Play Tennis\"][i]=='Yes':\n",
        "        county+=1\n",
        "print(county)\n",
        "countn=n-county\n",
        "PYN=list()\n",
        "PYN.append(county/n)\n",
        "PYN.append(1-PYN[0])\n",
        "print(PYN)\n",
        "at=att[1]\n",
        "csy=csn=coy=con=cry=crn=0\n",
        "col1=list(Train_data[att[1]].unique())\n",
        "for i in list(Train_data.index):\n",
        "    if Train_data[att[-1]][i]=='Yes':\n",
        "        if Train_data[at][i]==col1[0]:\n",
        "            csy+=1\n",
        "        elif Train_data[at][i]==col1[1]:\n",
        "            coy+=1\n",
        "        else:\n",
        "            cry+=1\n",
        "    else:\n",
        "        if Train_data[at][i]==col1[0]:\n",
        "            csn+=1\n",
        "        elif Train_data[at][i]==col1[1]:\n",
        "            con+=1\n",
        "        else:\n",
        "            crn+=1 \n",
        "OutlookY={col1[0]:csy/county,col1[1]:coy/county,col1[2]:cry/county}\n",
        "OutlookN={col1[0]:csn/countn,col1[1]:con/countn,col1[2]:crn/countn}\n",
        "print(OutlookY)\n",
        "print(OutlookN)\n",
        "list(Train_data['Outlook'].unique())\n",
        "at=att[2]\n",
        "chy=chn=cmy=cmn=ccy=ccn=0\n",
        "print(at)\n",
        "col1=list(Train_data[att[2]].unique())\n",
        "print(col1)\n",
        "for i in list(Train_data.index):\n",
        "    if Train_data[att[-1]][i]=='Yes':\n",
        "        if Train_data[at][i]==col1[0]:\n",
        "            chy+=1\n",
        "        elif Train_data[at][i]==col1[1]:\n",
        "            cmy+=1\n",
        "        else:\n",
        "            ccy+=1\n",
        "    else:\n",
        "        if Train_data[at][i]==col1[0]:\n",
        "            chn+=1\n",
        "        elif Train_data[at][i]==col1[1]:\n",
        "            cmn+=1\n",
        "        else:\n",
        "            ccn+=1 \n",
        "TempY={col1[0]:chy/county,col1[1]:cmy/county,col1[2]:ccy/county}\n",
        "TempN={col1[0]:chn/countn,col1[1]:cmn/countn,col1[2]:ccn/countn}\n",
        "print(TempY)\n",
        "print(TempN)\n",
        "at=att[3]\n",
        "cny=cnn=chy=chn=0\n",
        "print(at)\n",
        "col1=list(Train_data[att[3]].unique())\n",
        "print(col1)\n",
        "for i in list(Train_data.index):\n",
        "    if Train_data[att[-1]][i]=='Yes':\n",
        "        if Train_data[at][i]==col1[0]:\n",
        "            cny+=1\n",
        "        elif Train_data[at][i]==col1[1]:\n",
        "            chy+=1\n",
        "    else:\n",
        "        if Train_data[at][i]==col1[0]:\n",
        "            cnn+=1\n",
        "        elif Train_data[at][i]==col1[1]:\n",
        "            chn+=1\n",
        "HumidityY={col1[0]:cny/county,col1[1]:chy/county}\n",
        "HumidityN={col1[0]:cnn/countn,col1[1]:chn/countn}\n",
        "print(HumidityY)\n",
        "print(HumidityN)\n",
        "at=att[4]\n",
        "cwy=cwn=csy=csn=0\n",
        "print(at)\n",
        "col1=list(Train_data[att[4]].unique())\n",
        "print(col1)\n",
        "for i in list(Train_data.index):\n",
        "    if Train_data[att[-1]][i]=='Yes':\n",
        "        if Train_data[at][i]==col1[0]:\n",
        "            cwy+=1\n",
        "        elif Train_data[at][i]==col1[1]:\n",
        "            csy+=1\n",
        "    else:\n",
        "        if Train_data[at][i]==col1[0]:\n",
        "            cwn+=1\n",
        "        elif Train_data[at][i]==col1[1]:\n",
        "            csn+=1\n",
        "WindY={col1[0]:cwy/county,col1[1]:csy/county}\n",
        "WindN={col1[0]:cwn/countn,col1[1]:csn/countn}\n",
        "print(WindY)\n",
        "print(WindN)\n",
        "outa=[]\n",
        "for i in list(Test_data.index):\n",
        "    outa.append(Test_data['Play Tennis'][i])\n",
        "print(outa)\n",
        "out=list()\n",
        "ato=att[1:-1]\n",
        "print(ato)\n",
        "ind=Test_data.index.tolist()\n",
        "for i in ind:\n",
        "    prody=1\n",
        "    prodn=1\n",
        "    for j in ato:\n",
        "        if j=='Outlook':\n",
        "            prody=prody*OutlookY[Test_data[j][i]]\n",
        "            prodn=prodn*OutlookN[Test_data[j][i]]\n",
        "        elif j=='Temprature':\n",
        "            prody=prody*TempY[Test_data[j][i]]\n",
        "            prodn=prodn*TempN[Test_data[j][i]]\n",
        "        elif j=='Humidity':\n",
        "            prody=prody*HumidityY[Test_data[j][i]]\n",
        "            prodn=prodn*HumidityN[Test_data[j][i]]\n",
        "        elif j=='Wind':\n",
        "            prody=prody*WindY[Test_data[j][i]]\n",
        "            prodn=prodn*WindN[Test_data[j][i]]\n",
        "    if prody>prodn:\n",
        "        print(prody,prodn)\n",
        "        out.append(\"Yes\")\n",
        "    else:\n",
        "        out.append('No')\n",
        "cp=0\n",
        "for i in range(len(out)):\n",
        "    if out[i]==outa[i]:\n",
        "        cp+=1\n",
        "print(out)\n",
        "print(outa)\n",
        "print((cp/Test_data.shape[0])*100)\n"
      ],
      "metadata": {
        "id": "LcGAUnrSQf7b"
      },
      "id": "LcGAUnrSQf7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXpNo-4"
      ],
      "metadata": {
        "id": "yiDGdSfTUssU"
      },
      "id": "yiDGdSfTUssU"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
        "print(\"small x=\",x)\n",
        "#original output \n",
        "y = np.array(([92], [86], [89]), dtype=float)\n",
        "X = X/np.amax(X,axis=0) #maximum along the first axis \n",
        "print(\"Capital X\",X)\n",
        "#Defining Sigmoid Function for output \n",
        "def sigmoid (x):\n",
        "  return (1/(1 + np.exp(-x)))\n",
        "#Derivative of Sigmoid Function\n",
        "def derivatives_sigmoid(x):\n",
        "  return x * (1 - x)\n",
        "#Variables initialization\n",
        "epoch=7000 #Setting training iterations\n",
        "lr=0.1 #Setting learning rate\n",
        "inputlayer_neurons = 2 #number of input layer neurons \n",
        "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
        "output_neurons = 1 #number of neurons at output layer\n",
        "#Defining weight and biases for hidden and output layer \n",
        "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
        "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
        "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
        "bout=np.random.uniform(size=(1,output_neurons))\n",
        "#Forward Propagation \n",
        "for i in range(epoch):\n",
        "  hinp1=np.dot(X,wh)\n",
        "  hinp=hinp1 + bh\n",
        "  hlayer_act = sigmoid(hinp)\n",
        "  outinp1=np.dot(hlayer_act,wout)\n",
        "  outinp= outinp1+ bout\n",
        "  output = sigmoid(outinp)\n",
        "#Backpropagation Algorithm \n",
        "EO = y-output\n",
        "outgrad = derivatives_sigmoid(output)\n",
        "d_output = EO* outgrad\n",
        "EH = d_output.dot(wout.T)\n",
        "hiddengrad = derivatives_sigmoid(hlayer_act)\n",
        "#how much hidden layer wts contributed to error\n",
        "d_hiddenlayer = EH * hiddengrad\n",
        "wout += hlayer_act.T.dot(d_output) *lr\n",
        "# dotproduct of nextlayererror and currentlayerop\n",
        "bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
        "#Updating Weights\n",
        "wh += X.T.dot(d_hiddenlayer) *lr\n",
        "print(\"Actual Output: \\n\" + str(y))\n",
        "print(\"Predicted Output: \\n\" ,output)"
      ],
      "metadata": {
        "id": "qYp85F0sUurf"
      },
      "id": "qYp85F0sUurf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ExpNo-7"
      ],
      "metadata": {
        "id": "i30Nut7OVGHt"
      },
      "id": "i30Nut7OVGHt"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pgmpy\n",
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "from pgmpy.inference import VariableElimination\n",
        "#read Cleveland Heart Disease data\n",
        "heartDisease = pd.read_csv('/content/dive/MyDrive/heart.csv')\n",
        "heartDisease = heartDisease.replace('?',np.nan)\n",
        "#display the data\n",
        "print('Few examples from the dataset are given below')\n",
        "print(heartDisease.head())\n",
        "print('\\n Attributes and datatypes')\n",
        "print(heartDisease.dtypes)\n",
        "#Creat Model- Bayesian Network\n",
        "model = BayesianModel([('age','heartdisease'),('sex','heartdisease'),('exang','heartdisease'),('cp','heartdisease'),('heartdisease','restecg'),('heartdisease','chol')])  \n",
        "#Learning CPDs using Maximum Likelihood Estimators\n",
        "print('\\n Learning CPD using Maximum likelihood estimators')\n",
        "model.fit(heartDisease,estimator=MaximumLikelihoodEstimator)\n",
        "print(model.get_cpds('age'))\n",
        "print(model.get_cpds('exang'))\n",
        "print(model.get_cpds('sex'))\n",
        "print(model.get_cpds('cp'))\n",
        "print(model.get_cpds('restecg'))\n",
        "# Inferencing with Bayesian Network\n",
        "print('\\n Inferencing with Bayesian Network:')\n",
        "HeartDiseasetest_infer = VariableElimination(model)\n",
        "#computing the Probability of HeartDisease given restecg\n",
        "print('\\n 1.Probability of HeartDisease given evidence=restecg :1')\n",
        "q1=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'restecg':1})\n",
        "print(q1)\n",
        "#computing the Probability of HeartDisease given cp\n",
        "print('\\n 2.Probability of HeartDisease given evidence= cp:2 ')\n",
        "q2=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'cp':2})\n",
        "print(q2)\n",
        "print('\\n 3.Probability of HeartDisease given evidence= cp:3 ')\n",
        "q3=HeartDiseasetest_infer.query(variables=['heartdisease'],evidence={'cp':3})\n",
        "print(q3)"
      ],
      "metadata": {
        "id": "pqbVtRhbVI7i"
      },
      "id": "pqbVtRhbVI7i",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}